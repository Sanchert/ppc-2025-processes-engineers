# Сумма элементов вектора

- Студент: Колотухин Александр Дмитриевич, 3823Б1ПР2
- Технология: SEQ, MPI 
- Вариант: 1

## 1. Введение
Рассматривается задача суммирования элементов вектора. Работа предполагает разработку последовательной и MPI-параллельной версий алгоритма, сравнение их производительности на разных размерах входных данных и разном числе процессов.

## 2. Постановка задачи
На вход программе подается вектор из целых чисел длины N. Требуется найти сумму всех его элементов, это значение подается на выход.

**Ограничения:**
- N и элементы вектора являются беззнаковыи целыи 64-биными числами
- Программа должна корректно работать на одном и нескольких процессах

## 3. Описание базового алгоритма (последовательная версия)
Последовательный алгоритм — проход по всему вектору в цикле и накопление суммы в переменную. Время выполнения `O(N)`.
```cpp
for (uint64_t i = 0; i < s; i++) {
    sum += inputVec[i];
}
```
## 4. Схема распараллеливания
**Краткое описание**\
Код разделяет входной вектор между процессами, каждый считает сумму своей части, затем все суммы объединяются в одну через MPI_Allreduce, результат доступен у всех процессов.

**Каждый процесс:**
1. Получает идентификатор — `pid`, общее число процессов — `pCount` и ссылку на входной вектор — `inputVec`.
2. На основании своего номера, общего числа процессов и размера входного вектора определяет границы своих вычислений — `start`, `end`.*
3. В цикле суммирует элементы из вычисленного диапазона в локальную сумму — `local_sum`
4. Объединяет результат в глобальную сумму — `global_sum`
5. Глобальную сумму передает в выходное значение


## 5. Детали реализации
**Структура кода**
```text
kolotukhin_a_elem_vec_sum   
    │
    ├───common
    │   └───include
    │       └───common.hpp — Файл озадает типы используемых данных: воходные, выходные, тестовые
    │
    ├───mpi
    │   ├───include
    │   │   └───ops_mpi.hpp — Заголовочный файл параллелльной версии программы
    │   │
    │   └───src
    │       └───ops_mpi.cpp — Реализация параллельной версии
    │
    ├───seq
    │   ├───include
    │   │   └───ops_seq.hpp — Заголовочный файл последовательной версии программы
    │   │
    │   └───src
    │       └───ops_seq.cpp — реализация последовательной версии
    │
    ├───tests
    │   ├───functional
    │   │   └───main.cpp — Тесты оценки функционирования
    │   │
    │   └───performance
    │       └───main.cpp — Тесты оценки производительности
    │
    ├───info.json — Информация о выполнившем работу
    │
    ├───report.md — Отчет по работе (данный файл)
    │
    └───settings.json — Включение/выключение задачи
```

**Ключевые классы**
- `KolotukhinAElemVecSumMPI` — описание параллельного алгоритма
- `KolotukhinAElemVecSumSEQ` — описание последовательного алгоритма

**Важные предположения**
1. Входной вектор не пустой
2. Размеры входных данных подходят для обработки

**Использование памяти**
- В главном процессе создается и хранится входной вектор — использование памяти `O(N)`.
- В дочерних процессах используется ссылка на входной вектор — использование памяти `O(1)`.

Общее использовние памяти программой — `O(N)`.

## 6. Экспериментальная установка
- Аппаратное обеспечение / ОС 
    - Модель процессора: Intel Core 7 240H (10 ядер, 16 потоков);
    - Оперативная память: 16 ГБ;
    - Версия ОС: Windows 11 Home 25H2 (26200.6899).
- Инструменты 
    - Компилятор: MSVC версии 19.44.35217.0 (Visual Studio 2022);
    - Тип сборки: Release;
    - MPI: Microsoft MPI 10.1.12498.52.
- Окружение
    - Количество процессов: 1, 2, 4, 8.
- Данные
    - В функциональных тестах содержится массив чисел — длины векторов. Вектора заполняются числами от `0` до `N-1`.
    - В тестах производительности создается вектор размером `100000000` элементов из единиц.

## 7. Результаты

### 7.1 Корректность
Проверка осуществляется с помощью Google Test:

6 функциональных тестов покрывают:
- минимально возможное значение длины вектора
- оработка маленьких векторов
- оработка больших векторов
- большие монотонные последовательности

Текущая реализация проходит функциональное тестирование

### 7.2 Эксплуатация
Результаты performance тестов при входном векторе длины `100000000`.

Описание результата:
- Выполняющая версия алгоритма
- Число участвующих процессов
- Время работы вычислительной части в секундах
- Ускорение = время seq версии / время mpi версии
- Эффективность = (Ускорение / Процессы) * 100 %

| Версия      | Процессы | Время, с | Ускорение | Эффективность |
|-------------|----------|----------|-----------|---------------|
| seq         | 1        | 0.772    | 1.00      | N/A           |
| mpi         | 2        | 0.642    | 1.20      | 60.0%         |
| mpi         | 4        | 0.820    | 0.94      | 23.5%         |
| mpi         | 8        | 4.325    | 0.17      | 2.125%        |

## 8. Заключение
На основании проведенных экспериментов и их анализа можно сделать следующие выводы:

1. Корректность реализации

Алгоритм успешно справляется с вычислениями, включая большие объемы данных (100 миллионов элементов).

2. Производительность и масштабируемость

- Время выполнения последовательной версии составляет около 0.772 секунд для вектора из 100 миллионов элементов.
- Наблюдается ухудшение времени при увеличении количества процессов, что объясняется реализацией: низкозатратные вычисления не перекрывают накладных расходов на организацию сообщений между процессами — `Allreduce()`.
- Предыдущий вывод позволяет предположить, что увеличение числа суммируемых элементов повысит относительную эффективность mpi версии (болше расходов на вычисления при тех же расходах на сообщения).

**Общий вывод:**

Реализованный параллельный алгоритм успешно решает задачу суммирования элементов вектора и демонстрирует хорошие показатели ускорения при небольшом числе процессов.

## 9. Источники
1. Документация по курсу «Параллельное программирование» / Parallel Programming Course [Электронный ресурс]. — Режим доступа: https://learning-process.github.io/parallel_programming_course/ru/index.html. — Дата обращения: 13.11.2025.
2. Kendall W. MPI Tutorial [Электронный ресурс]. — URL: https://mpitutorial.com. — Дата обращения: 15.11.2025.
3. Сысоев А. В. Курс лекций по параллельному программированию [Электронный ресурс]. — URL: https://source.unn.ru. — Требуется авторизация. — Дата обращения: 15.11.2025.

## Приложение
```cpp
bool KolotukhinAElemVecSumMPI::RunImpl() {
  int pid = 0;
  int p_count = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &pid);
  MPI_Comm_size(MPI_COMM_WORLD, &p_count);

  const std::vector<std::uint64_t> &input_vec = GetInput();
  auto uint_pid = static_cast<std::uint64_t>(pid);
  auto uint_p_count = static_cast<std::uint64_t>(p_count);
  std::uint64_t vector_size = input_vec.size();
  std::uint64_t min_part = vector_size / uint_p_count;
  std::uint64_t proc_size = min_part + (std::less<>()(uint_pid, vector_size % uint_p_count) ? 1 : 0);
  std::uint64_t rem = vector_size % uint_p_count;
  std::uint64_t local_sum = 0;
  std::uint64_t start = (min_part * uint_pid) + (std::less<>()(uint_pid, rem) ? uint_pid : rem);
  std::uint64_t end = start + proc_size;
  for (std::uint64_t i = start; (i < end) && (i < input_vec.size()); i++) {
    local_sum += input_vec[i];
  }
  std::uint64_t global_sum = 0;

  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);
  GetOutput() = global_sum;
  return true;
}
```